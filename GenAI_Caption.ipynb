{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the input JSON file\n",
        "try:\n",
        "    with open('sports1.json', 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Extract caption and hashtags from posts in each user profile\n",
        "    filtered_data = []\n",
        "\n",
        "    for profile in data:\n",
        "        if 'latestIgtvVideos' in profile and isinstance(profile['latestIgtvVideos'], list):\n",
        "            for post in profile['latestIgtvVideos']:\n",
        "                if isinstance(post, dict) and 'caption' in post and 'hashtags' in post:\n",
        "                    filtered_data.append({\n",
        "                        'caption': post['caption'],\n",
        "                        'hashtags': post['hashtags']\n",
        "                    })\n",
        "\n",
        "    print(f\"Processed {len(filtered_data)} entries\")\n",
        "\n",
        "    # Save the filtered data to a new JSON file\n",
        "    with open('filtered_output.json', 'w', encoding='utf-8') as file:\n",
        "        json.dump(filtered_data, file, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"Filtered data saved to 'filtered_output.json'\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'food1.json' was not found. Check your file path.\")\n",
        "except json.JSONDecodeError:\n",
        "    print(\"Error: The file is not valid JSON. Check your JSON syntax.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKMTLlBxXyqy",
        "outputId": "56564ded-b38e-4a8d-b944-de73ed3f1da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 104 entries\n",
            "Filtered data saved to 'filtered_output.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Replace 'abc' with your actual path\n",
        "input_path = '/content/food1.json'\n",
        "output_path = 'filtered_output.json'\n",
        "\n",
        "# Read the original JSON file\n",
        "with open(input_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract only entries where both caption and hashtags are non-empty\n",
        "filtered_data = []\n",
        "for item in data:\n",
        "    caption = item.get('caption', '').strip()\n",
        "    hashtags = item.get('hashtags', [])\n",
        "\n",
        "    if caption and hashtags:  # Both must be non-empty\n",
        "        filtered_data.append({\n",
        "            'caption': caption,\n",
        "            'hashtags': hashtags\n",
        "        })\n",
        "\n",
        "# Write the filtered data to a new file\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(filtered_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Filtered JSON saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "xzBx6X4pZUFU",
        "outputId": "f288ba93-72bf-4abc-cb57-5f9d433a6a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/food1.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-36eaed0c42d8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Read the original JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/food1.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langdetect\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c25YA5ZZdih",
        "outputId": "16bcab6c-a979-4677-d401-2fd2eb33d261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from langdetect import detect, LangDetectException\n",
        "\n",
        "input_path = 'filtered_output.json'\n",
        "output_path = 'english_only_output.json'\n",
        "\n",
        "def is_english(text):\n",
        "    try:\n",
        "        return detect(text) == 'en'\n",
        "    except LangDetectException:\n",
        "        return False\n",
        "\n",
        "with open(input_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "cleaned_data = []\n",
        "for item in data:\n",
        "    raw_caption = item.get('caption', '')\n",
        "    raw_hashtags = item.get('hashtags', [])\n",
        "\n",
        "    # Remove #words from caption\n",
        "    cleaned_caption = ' '.join(word for word in raw_caption.split() if not word.startswith('#')).strip()\n",
        "\n",
        "    # Filter hashtags: keep only English ones\n",
        "    cleaned_hashtags = [tag for tag in raw_hashtags if is_english(tag)]\n",
        "\n",
        "    # Keep only if both caption and hashtags are non-empty and in English\n",
        "    if cleaned_caption and cleaned_hashtags and is_english(cleaned_caption):\n",
        "        cleaned_data.append({\n",
        "            'caption': cleaned_caption,\n",
        "            'hashtags': cleaned_hashtags\n",
        "        })\n",
        "\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(cleaned_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Cleaned English-only captions saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDW12xeWaQ4r",
        "outputId": "db03a474-c346-4efe-8699-333347f7805d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned English-only captions saved to english_only_output.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "input_path = '/content/english_only_output.json'\n",
        "output_path = '/content/final_cleaned_output.json'\n",
        "\n",
        "# Load data\n",
        "with open(input_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Remove @mentions from captions\n",
        "cleaned_data = []\n",
        "for item in data:\n",
        "    caption = item.get('caption', '')\n",
        "    hashtags = item.get('hashtags', [])\n",
        "\n",
        "    # Remove words starting with '@'\n",
        "    cleaned_caption = ' '.join(word for word in caption.split() if not word.startswith('@')).strip()\n",
        "\n",
        "    # Skip if caption or hashtags end up empty\n",
        "    if cleaned_caption and hashtags:\n",
        "        cleaned_data.append({\n",
        "            'caption': cleaned_caption,\n",
        "            'hashtags': hashtags\n",
        "        })\n",
        "\n",
        "# Save cleaned data\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(cleaned_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"@mentions removed and saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuEO6k09cUmk",
        "outputId": "0ea086ff-6de1-4cad-dd04-1a4c748bb26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@mentions removed and saved to /content/final_cleaned_output.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKVoEiXAcvZq",
        "outputId": "32c6801f-427f-4755-9491-269c9dcb3302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import spacy\n",
        "\n",
        "input_path = '/content/final_cleaned_output.json'\n",
        "output_path = '/content/ner_anon_output.json'\n",
        "\n",
        "# Load English NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "with open(input_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "ner_data = []\n",
        "\n",
        "for item in data:\n",
        "    caption = item['caption']\n",
        "    hashtags = item['hashtags']\n",
        "\n",
        "    doc = nlp(caption)\n",
        "\n",
        "    # Replace ORG entities with <ORG>\n",
        "    new_caption = caption\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"ORG\":\n",
        "            new_caption = new_caption.replace(ent.text, \"<ORG>\")\n",
        "\n",
        "    ner_data.append({\n",
        "        'caption': new_caption,\n",
        "        'hashtags': hashtags\n",
        "    })\n",
        "\n",
        "# Save the updated JSON\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(ner_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"NER processed captions saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT3g5QnsdCTl",
        "outputId": "5b04f2e9-1109-4895-89b3-df8c0c2d1f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER processed captions saved to /content/ner_anon_output.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load the SentenceTransformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Provided hashtags list\n",
        "hashtags = [\n",
        "    # General Popular Hashtags\n",
        "    \"#love\", \"#instagood\", \"#photooftheday\", \"#fashion\", \"#beautiful\", \"#happy\", \"#cute\",\n",
        "    \"#tbt\", \"#like4like\", \"#followme\", \"#picoftheday\", \"#follow\", \"#me\", \"#selfie\", \"#summer\",\n",
        "    \"#art\", \"#instadaily\", \"#friends\", \"#repost\", \"#nature\", \"#girl\", \"#fun\", \"#style\",\n",
        "    \"#smile\", \"#food\", \"#instalike\", \"#likeforlike\", \"#family\", \"#travel\", \"#fitness\",\n",
        "    \"#igers\", \"#tagsforlikes\", \"#follow4follow\", \"#nofilter\", \"#life\", \"#beauty\", \"#amazing\",\n",
        "    \"#instamood\", \"#instagram\", \"#photography\", \"#vscocam\", \"#sun\", \"#photo\", \"#music\",\n",
        "    \"#beach\", \"#followforfollow\", \"#bestoftheday\", \"#sky\", \"#ootd\", \"#sunset\", \"#dog\", \"#vsco\",\n",
        "    \"#l4l\", \"#makeup\", \"#f4f\", \"#foodporn\", \"#hair\", \"#pretty\", \"#swag\", \"#cat\", \"#model\",\n",
        "    \"#motivation\", \"#baby\", \"#party\", \"#cool\", \"#lol\", \"#gym\", \"#design\", \"#instapic\",\n",
        "    \"#funny\", \"#healthy\", \"#night\", \"#tflers\", \"#yummy\", \"#flowers\", \"#lifestyle\", \"#hot\",\n",
        "    \"#instafood\", \"#wedding\", \"#fit\", \"#handmade\", \"#black\", \"#pink\", \"#blue\", \"#work\",\n",
        "    \"#workout\", \"#blackandwhite\", \"#drawing\", \"#inspiration\", \"#home\", \"#holiday\",\n",
        "    \"#christmas\", \"#nyc\", \"#london\", \"#sea\", \"#instacool\", \"#goodmorning\", \"#iphoneonly\",\n",
        "    \"#contest\", \"#giveaway\", \"#competition\", \"#win\",\n",
        "\n",
        "    # Travel & Adventure\n",
        "    \"#wanderlust\", \"#adventure\", \"#explore\", \"#travelgram\", \"#naturelovers\", \"#hiking\",\n",
        "    \"#roadtrip\", \"#vacation\", \"#backpacking\", \"#sunrise\", \"#sunsetlovers\", \"#mountains\",\n",
        "    \"#camping\", \"#ocean\", \"#islandlife\", \"#paradise\", \"#nationalpark\", \"#cityscape\",\n",
        "\n",
        "    # Fitness & Health\n",
        "    \"#fitfam\", \"#gymlife\", \"#workoutmotivation\", \"#running\", \"#yoga\", \"#healthylifestyle\",\n",
        "    \"#strength\", \"#muscle\", \"#nutrition\", \"#calisthenics\", \"#strong\", \"#gains\", \"#powerlifting\",\n",
        "    \"#crossfit\", \"#vegan\", \"#plantbased\", \"#cleaneating\",\n",
        "\n",
        "    # Fashion & Style\n",
        "    \"#streetstyle\", \"#mensfashion\", \"#womensfashion\", \"#styleinspo\", \"#ootdfashion\", \"#trend\",\n",
        "    \"#fashionblogger\", \"#accessories\", \"#luxury\", \"#highfashion\", \"#vintage\", \"#boho\",\n",
        "    \"#denim\", \"#minimaliststyle\",\n",
        "\n",
        "    # Tech & Gaming\n",
        "    \"#technology\", \"#gadget\", \"#techlife\", \"#coding\", \"#developer\", \"#programming\",\n",
        "    \"#AI\", \"#machinelearning\", \"#gamers\", \"#esports\", \"#gamingcommunity\", \"#console\",\n",
        "    \"#gaminglife\", \"#nft\", \"#blockchain\", \"#crypto\",\n",
        "\n",
        "    # Mental Health & Wellness\n",
        "    \"#mentalhealth\", \"#selfcare\", \"#positivity\", \"#mindfulness\", \"#healing\", \"#breathe\",\n",
        "    \"#depressionawareness\", \"#anxietyrelief\", \"#meditation\", \"#therapy\", \"#kindness\",\n",
        "\n",
        "    # Relationships & Family\n",
        "    \"#couplegoals\", \"#relationshipgoals\", \"#bestfriend\", \"#siblings\", \"#happilyeverafter\",\n",
        "    \"#parenting\", \"#momlife\", \"#dadlife\", \"#brother\", \"#sister\",\n",
        "\n",
        "    # Animals & Pets\n",
        "    \"#dogsofinstagram\", \"#catsofinstagram\", \"#petstagram\", \"#wildlife\", \"#puppylove\",\n",
        "    \"#rescuedog\", \"#adoptdontshop\", \"#exoticpets\",\n",
        "\n",
        "    # Hobbies & Creative\n",
        "    \"#handlettering\", \"#pottery\", \"#woodworking\", \"#diy\", \"#painting\", \"#poetry\",\n",
        "    \"#calligraphy\", \"#crafting\", \"#sketching\", \"#artsy\", \"#cinematography\",\n",
        "\n",
        "    # Food & Drinks\n",
        "    \"#coffeelover\", \"#brunch\", \"#homemade\", \"#dessert\", \"#chocolatelover\", \"#baking\",\n",
        "    \"#smoothiebowl\", \"#wine\", \"#cocktails\",\n",
        "\n",
        "    # Cars & Motorcycles\n",
        "    \"#carporn\", \"#carlifestyle\", \"#supercars\", \"#motorcycle\", \"#bikelife\", \"#offroading\",\n",
        "    \"#classiccars\", \"#trucklife\",\n",
        "\n",
        "    # Events & Holidays\n",
        "    \"#newyears\", \"#valentines\", \"#halloween\", \"#thanksgiving\", \"#easter\", \"#diwali\",\n",
        "    \"#ramadan\", \"#hanukkah\", \"#birthday\", \"#graduation\",\n",
        "\n",
        "    # Motivational & Business\n",
        "    \"#hustle\", \"#entrepreneur\", \"#successmindset\", \"#startup\", \"#billionairemindset\",\n",
        "    \"#sidehustle\", \"#wealth\", \"#bosslife\", \"#productivity\", \"#marketing\", \"#freelancer\",\n",
        "\n",
        "    # Science & Space\n",
        "    \"#science\", \"#astronomy\", \"#astrophysics\", \"#spaceexploration\", \"#NASA\", \"#cosmos\",\n",
        "    \"#futuretech\", \"#quantumphysics\",\n",
        "\n",
        "    # Photography & Content Creation\n",
        "    \"#photochallenge\", \"#mobilephotography\", \"#dronephotography\", \"#filmphotography\",\n",
        "    \"#portraitmode\", \"#streetphotography\", \"#cinematic\", \"#dslr\",\n",
        "\n",
        "    # Music & Entertainment\n",
        "    \"#livemusic\", \"#musiclover\", \"#concert\", \"#piano\", \"#guitar\", \"#vinyl\", \"#dj\",\n",
        "    \"#hiphop\", \"#rockmusic\", \"#indie\", \"#festival\",\n",
        "\n",
        "    # Books & Learning\n",
        "    \"#bookstagram\", \"#bibliophile\", \"#readinglist\", \"#quotesoftheday\", \"#philosophy\",\n",
        "    \"#history\", \"#education\", \"#lifelonglearning\", \"#selfimprovement\",\n",
        "\n",
        "    # Spiritual & Astrology\n",
        "    \"#spiritualawakening\", \"#zodiacsigns\", \"#horoscope\", \"#meditationpractice\",\n",
        "    \"#lawofattraction\", \"#chakras\", \"#manifestation\",\n",
        "\n",
        "    # Environmental & Sustainability\n",
        "    \"#ecofriendly\", \"#sustainableliving\", \"#climatechange\", \"#zerowaste\", \"#recycle\",\n",
        "    \"#veganlife\", \"#organic\", \"#savetheplanet\",\n",
        "\n",
        "    # Parenting & Kids\n",
        "    \"#babyfashion\", \"#toddlermom\", \"#dadjokes\", \"#momsofinstagram\", \"#parenthood\",\n",
        "    \"#raisingkids\", \"#babyboy\", \"#babygirl\",\n",
        "\n",
        "    # Local & City Hashtags\n",
        "    \"#paris\", \"#newyorkcity\", \"#losangeles\", \"#dubai\", \"#tokyo\", \"#berlin\", \"#sydney\",\n",
        "    \"#mumbai\", \"#toronto\", \"#brazil\", \"#bali\", \"#europe\", \"#latinamerica\",\n",
        "\n",
        "    # Miscellaneous Trends\n",
        "    \"#trending\", \"#viral\", \"#instalove\", \"#instafun\", \"#instasuccess\", \"#epic\",\n",
        "    \"#hype\", \"#squad\", \"#random\", \"#tropical\"\n",
        "    # Special Days & Awareness\n",
        "    \"#worldenvironmentday\", \"#internationalwomensday\", \"#earthday\", \"#worldmentalhealthday\",\n",
        "    \"#breastcancerawareness\", \"#worldaidsday\", \"#blackhistorymonth\"\n",
        "]\n",
        "\n",
        "# Encode all hashtags once\n",
        "hashtag_embeddings = model.encode(hashtags, convert_to_tensor=True)\n",
        "\n",
        "# Load the existing travel2.json file\n",
        "with open(\"/content/ner_anon_output.json\", \"r\") as f:\n",
        "    posts = json.load(f)\n",
        "\n",
        "# Number of new hashtags to add\n",
        "TOP_N = 5\n",
        "\n",
        "# Update each post\n",
        "for post in posts:\n",
        "    caption = post.get(\"caption\", \"\")\n",
        "    existing_hashtags = post.get(\"hashtags\", [])\n",
        "\n",
        "    # Encode the caption\n",
        "    caption_embedding = model.encode(caption, convert_to_tensor=True)\n",
        "\n",
        "    # Compute similarity scores\n",
        "    scores = util.pytorch_cos_sim(caption_embedding, hashtag_embeddings)[0]\n",
        "\n",
        "    # Get indices of top N similar hashtags\n",
        "    top_indices = scores.argsort(descending=True)[:TOP_N]\n",
        "\n",
        "    # Extract top hashtags (without # prefix for consistency with existing)\n",
        "    new_hashtags = [hashtags[i][1:] for i in top_indices]\n",
        "\n",
        "    # Combine and remove duplicates while preserving order\n",
        "    updated_hashtags = list(dict.fromkeys(existing_hashtags + new_hashtags))\n",
        "    post[\"hashtags\"] = updated_hashtags\n",
        "\n",
        "# Save back to travel2.json\n",
        "with open(\"/content/travel2.json\", \"w\") as f:\n",
        "    json.dump(posts, f, indent=2)\n"
      ],
      "metadata": {
        "id": "xSRr1ONse-4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eQjihrf3jEsS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}